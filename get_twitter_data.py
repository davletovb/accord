# -*- coding: utf-8 -*-
"""Twitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sQ-TNsOoIyabl4bhst0A9SSv1YfleY0X
"""

# import lots of stuff
import sys
import os
import re
import tweepy
from tweepy import OAuthHandler

import numpy as np
import pandas as pd
from datetime import datetime, timedelta

from os import path
from PIL import Image
from wordcloud import WordCloud, STOPWORDS

from google.colab import drive
drive.mount('/content/gdrive')

#@title Enter Twitter Credentials
TWITTER_KEY = 'rxZKr5xZ9S1b6bG4jIVXVkZqu' #@param {type:"string"}
TWITTER_SECRET_KEY = 'fX1wkeXC9x7y9TcrZyBZx9b6LbVjh0500geu81ysMKpNSDkW2k' #@param {type:"string"}

auth = tweepy.AppAuthHandler(TWITTER_KEY, TWITTER_SECRET_KEY)

api = tweepy.API(auth, wait_on_rate_limit=True,
				   wait_on_rate_limit_notify=True)

if (not api):
    print ("Can't Authenticate")
    sys.exit(-1)

maxTweets = 1000
Filter_Retweets = True

tweet_list=[]
for tweet in tweepy.Cursor(api.user_timeline, id="balajis", include_rts=False, tweet_mode="extended").items(maxTweets):
    #print(tweet.text)
    tweet_list.append([tweet.created_at.date(), 
                      tweet.id, tweet.user.screen_name, tweet.user.name, tweet.user.id, tweet.full_text, tweet.favorite_count, 
                      ])
    
#clear_output()
print("Downloaded {0} tweets".format(len(tweet_list)))

pd.set_option('display.max_colwidth', -1)

# load it into a pandas dataframe
tweet_df = pd.DataFrame(tweet_list, columns=['tweet_date', 'tweet_id', 'username', 'name', 'user_id', 'tweet', 'like_count'])
tweet_df.head()

#Create a column for hashtags
tweet_df['hashtag'] = tweet_df['tweet'].apply(lambda x: re.findall(r'\B#\w*[a-zA-Z]+\w*', x))
tweet_df.head(10)

!pip install tweet-preprocessor

import preprocessor as prep

prep.set_options(prep.OPT.URL, prep.OPT.EMOJI, prep.OPT.MENTION, prep.OPT.SMILEY, prep.OPT.RESERVED, prep.OPT.NUMBER)

cleaned = [prep.clean(text) for text in tweet_df['tweet']]

print(cleaned)

#!python -m nltk.downloader all
#!pip install unidecode

import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
from unidecode import unidecode
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

import string

lemmatizer = WordNetLemmatizer()

def lemmatize(corpus):
    lemmatized=[]
    tokens=word_tokenize(corpus)
    for token in tokens:
      if len(token)>2:
        #stemmed_token=stemmer.stem(token)
        lemmatized_token1=lemmatizer.lemmatize(token, pos="n")
        lemmatized_token2=lemmatizer.lemmatize(lemmatized_token1, pos="v")
        lemmatized_token3=lemmatizer.lemmatize(lemmatized_token2, pos="a")
        lemmatized_token=lemmatizer.lemmatize(lemmatized_token3, pos="r")
        lemmatized.append(lemmatized_token)
      elif len(token)==2:
        lemmatized.append(token)
      else:
        lemmatized.append("")
    return " ".join([i for i in lemmatized])

def pre_process(corpus):
    # convert input corpus to lower case.
    corpus = corpus.lower()
    corpus = lemmatize(corpus)
    # collecting a list of stop words from nltk and punctuation form
    # string class and create single array.
    stopset = stopwords.words('english') + list(string.punctuation)
    # remove stop words and punctuations from string.
    # word_tokenize is used to tokenize the input corpus in word tokens.
    corpus = " ".join([i for i in word_tokenize(corpus) if i not in stopset])
    # remove digits
    corpus = "".join([i for i in corpus if not i.isdigit()])
    # remove non-ascii characters
    corpus = unidecode(corpus)
    return corpus

pre_processed = [pre_process(tweet) for tweet in cleaned]

print(pre_processed)

lemmatizer = WordNetLemmatizer()
punctuations = '''!()-=![]{};:+'`"\,<>./?@#$%^&*_~'''

def lemmatize(token):
    lemmatized_token1=lemmatizer.lemmatize(token, pos="n")
    lemmatized_token2=lemmatizer.lemmatize(lemmatized_token1, pos="v")
    lemmatized_token3=lemmatizer.lemmatize(lemmatized_token2, pos="a")
    lemmatized_token=lemmatizer.lemmatize(lemmatized_token3, pos="r")
    return lemmatized_token

def pre_process(corpus):
    corpus = corpus.lower()
    stopset = stopwords.words('english') + list(punctuations)
    tokens = word_tokenize(corpus)
    cleaned_corpus=[]
    for token in tokens:
      if ((token not in stopset) or (not token.isdigit)):
        lemmatized_token = lemmatize(token)
        cleaned_token = unidecode(lemmatized_token)
        cleaned_corpus.append(cleaned_token)
      else:
        continue
    return cleaned_corpus

pre_processed = [pre_process(tweet) for tweet in cleaned]

print(pre_processed)

#Normalise words
#!pip install num2words
#from num2words import num2words
#preprocessed = [''.join(re.sub(r'\d', num2words(text))) for text in cleaned]
#!python -m nltk.downloader all
#!pip install normalise
from normalise import normalise
from nltk.tokenize import word_tokenize

custom_abbr = {
    "FB":"Facebook",
    "AR":"Augmented Reality",
    "VR":"Virtual Reality",
    "AI":"Artificial Intelligence",
    "ML":"Machine Learning"
}

preprocessed = [' '.join(normalise(text, tokenizer=word_tokenize, user_abbrevs=custom_abbr, verbose=False)) for text in pre_processed]
print(preprocessed)

#Remove punctuations
import string
punctuations = '''!()-=![]{};:+'`"\,<>./?@#$%^&*_~'''

regex = re.compile('[%s]' % re.escape(punctuations))

pre_final = [' '.join(regex.sub(u' ', text).split()) for text in pre_processed]

print(pre_final)

from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

lemmatized=[]
for tweet in cleaned:
  words = []
  tokens = word_tokenize(tweet)
  for token in tokens:
    if len(token)>2:
      #stemmed_token=stemmer.stem(token)
      lemmatized_token1=lemmatizer.lemmatize(token, pos="n")
      lemmatized_token2=lemmatizer.lemmatize(lemmatized_token1, pos="v")
      lemmatized_token3=lemmatizer.lemmatize(lemmatized_token2, pos="a")
      lemmatized_token=lemmatizer.lemmatize(lemmatized_token3, pos="r")
      words.append(lemmatized_token)
    elif len(token)==2:
      words.append(token)
    else:
      continue
  lemmatized.append(words)

print(lemmatized)

#Add cleaned text as new column

tweet_df['cleaned'] = cleaned
tweet_df['tokens'] = pre_processed

tweet_new = tweet_df[tweet_df['cleaned'].str.split().str.len()>1]

tweet_new.head(10)

tweet_new.to_csv('tweets.csv')
!cp tweets.csv "/content/gdrive/My Drive/"
