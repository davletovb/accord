# -*- coding: utf-8 -*-
"""Twitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sQ-TNsOoIyabl4bhst0A9SSv1YfleY0X
"""

# import lots of stuff
import sys
import os
import re
import tweepy
from tweepy import OAuthHandler

import numpy as np
import pandas as pd

from os import path

import re
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances

import gensim.downloader as api
from gensim.models.word2vec import Word2Vec

!pip install sentence_transformers
!pip install tweet-preprocessor
!pip install unidecode
!pip install emoji
!python -m spacy download en_core_web_lg
!python -m nltk.downloader all

from sentence_transformers import SentenceTransformer
import preprocessor as prep

import string
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
from unidecode import unidecode
from nltk.stem import WordNetLemmatizer
#from nltk.tokenize import word_tokenize
from nltk.tokenize import TweetTokenizer

from google.colab import drive
drive.mount('/content/gdrive')

#@title Enter Twitter Credentials
TWITTER_KEY = 'rxZKr5xZ9S1b6bG4jIVXVkZqu' #@param {type:"string"}
TWITTER_SECRET_KEY = 'fX1wkeXC9x7y9TcrZyBZx9b6LbVjh0500geu81ysMKpNSDkW2k' #@param {type:"string"}

auth = tweepy.AppAuthHandler(TWITTER_KEY, TWITTER_SECRET_KEY)

api = tweepy.API(auth, wait_on_rate_limit=True,
				   wait_on_rate_limit_notify=True)

if (not api):
    print ("Can't Authenticate")
    sys.exit(-1)

max_tweets = 1000

tweet_list=[]
for tweet in tweepy.Cursor(api.user_timeline, id="balajis", include_rts=False, tweet_mode="extended").items(max_tweets):

    tweet_list.append([ 
                      tweet.id, tweet.created_at.date(), tweet.user.screen_name, tweet.user.name, tweet.user.id, tweet.favorite_count, tweet.retweet_count, tweet.full_text
                      ])

print("Downloaded {0} tweets".format(len(tweet_list)))

pd.set_option('display.max_colwidth', -1)

# load it into a pandas dataframe
tweet_df = pd.DataFrame(tweet_list, columns=['tweet_id', 'tweet_date', 'username', 'name', 'user_id', 'like_count', 'retweet_count', 'text'])
tweet_df.head()

#Create a column for hashtags
tweet_df.insert(loc=7, column='hashtag', value=tweet_df['text'].apply(lambda x: re.findall(r'\B#\w*[a-zA-Z]+\w*', x)))
tweet_df.head(10)

def remove_emojis(data):
    emoj = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002500-\U00002BEF"  # chinese char
        u"\U00002702-\U000027B0"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642" 
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # dingbats
        u"\u3030"
                      "]+", re.UNICODE)
    return re.sub(emoj, '', data)

stopset = stopwords.words('english')
punctuations = '''!()-=—!→–[]|{};:+`"“”\,<>/@#$%^&*_~'''

prep.set_options(prep.OPT.URL, prep.OPT.MENTION)

cleaned = [prep.clean(text) for text in tweet_df['text']]

print(cleaned)

tweet_df['cleaned'] = cleaned

#tweet_df['cleaned'] = tweet_df.cleaned.apply(lambda x: " ".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stopset))

tweet_df['cleaned'] = tweet_df.cleaned.apply(lambda x: x.translate(str.maketrans(punctuations, ' '*len(punctuations))))
tweet_df['cleaned'] = tweet_df.cleaned.apply(lambda x: remove_emojis(x))
tweet_df['cleaned'] = tweet_df.cleaned.apply(lambda x: x.replace('...',' '))
tweet_df['cleaned'] = tweet_df.cleaned.apply(lambda x: ' '.join(x.split()))


tweet_df.head(10)

tokenizer = TweetTokenizer()
lemmatizer = WordNetLemmatizer()
punctuations = '''!0123456789()-=—!→–[]{};:+`'"“”\,<>.?/@#$%^&*_~'''
stopset = stopwords.words('english')

abbrevs = {
    'dm': 'direct message',
    'ai':'artificial intelligence',
    'ar':'augmented reality',
    'vr':'virtual reality',
    'ml':'machine learning',
    'btc':'bitcoin',
    'eth':'ethereum'
}

def lemmatize(token):
    lemmatized_token1=lemmatizer.lemmatize(token, pos="v")
    lemmatized_token2=lemmatizer.lemmatize(lemmatized_token1, pos="n")
    lemmatized_token3=lemmatizer.lemmatize(lemmatized_token2, pos="a")
    lemmatized_token=lemmatizer.lemmatize(lemmatized_token3, pos="r")
    return lemmatized_token

def pre_process(corpus):
    corpus = corpus.lower()
    corpus = corpus.translate(str.maketrans(punctuations, ' '*len(punctuations)))
    tokens = tokenizer.tokenize(corpus)
    cleaned_corpus=[]
    for token in tokens:
      token = ''.join([i for i in token if not i.isdigit()])
      if ((token not in stopset) and (len(token)>2)):
        token = lemmatize(token)
        token = token.translate(str.maketrans('', '', string.punctuation))
        token = unidecode(token)
        cleaned_corpus.append(token)
      else:
        continue
    return cleaned_corpus

pre_processed = [pre_process(tweet) for tweet in cleaned]

print(pre_processed)

#Add cleaned text as new column
#punctuations = '''0123456789!()-=![]{};:.?+`'“”"\,<>/@#$%^&*_~'''
#cleaned = [text.lower().translate(str.maketrans(punctuations, ' '*len(punctuations))) for text in cleaned]

#tweet_df['cleaned'] = cleaned
tweet_df['tokens'] = pre_processed

#tweet_new = tweet_df[tweet_df['cleaned'].str.split().str.len()>1]

tweet_df.head(10)

docs = []
doc = ""
words = ""

cleaned = tweet_df['cleaned'].tolist()
tokens = [item for sublist in pre_processed for item in sublist]
words = " ".join(tokens)

doc = " ".join(cleaned)

docs.append([tweet_list[0][4], tweet_list[0][2], doc, words])

print(docs)

doc_df = pd.DataFrame(docs, columns=["userid","username","tweets","words"])

doc_df.head(5)

tweet_df.to_csv('tweets.csv')
doc_df.to_csv('doc.csv')
!cp tweets.csv "/content/gdrive/My Drive/"
!cp doc.csv "/content/gdrive/My Drive/"

import tensorflow_hub as hub

embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder-large/5")
#embeddings = embed([doc])

#print(embeddings)

class MeanEmbeddingVectorizer(object):
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        if len(tokenizer)>0:
          self.dim=20 #len(tokenizer[next(iter("test"))])
        else:
          self.dim=0
            
    def fit(self, X, y):
        return self 

    def transform(self, X):
        return np.array([
            np.mean([torch.tensor([self.tokenizer.encode(sentence)]) for sentence in sentences] 
                    or [np.zeros(self.dim)], axis=0)
            for sentences in X
        ])

#tweets = tweet_df['cleaned'].tolist()

#for tweet in tweets:
  #print(tokenizer2.encode(tweet))
#meanvec = MeanEmbeddingVectorizer(tokenizer2)
#meanvec.transform(tweet_df['cleaned'].tolist())

#meanvec = np.mean([embed([tweet]) for tweet in tweets], axis=0)
#print(meanvec)

tweets = tweet_df['cleaned'].tolist()

vecs = [embed([tweet]) for tweet in tweets]

vecs = np.array(vecs)

#vecs = vecs.reshape((vecs.shape[0], -1), order='F')
#vecs.shape

mean_vec_transformer = np.array([np.mean(vecs, axis=0)])
print(mean_vec_transformer)

#!python -m spacy download en_core_web_lg
import spacy


nlp = spacy.load("en_core_web_lg")

all_words = set(word for word in tokens)
print(tokens)
#word2vectors = {}
#for word in tokens:
#  if word in all_words:
#    nums=np.array(nlp(word).vector, dtype=np.float32)
#    word2vectors[word] = nums

print(all_words)

tfidf = TfidfVectorizer()
tfidf.fit(tokens)
word2weight = dict(zip(tfidf.get_feature_names(), tfidf.idf_))
print(word2weight)

word2vec = {}

for word in all_words:
  word2vec[word] = nlp(word).vector

mean_vec_tfidf = np.array([np.mean([word2vec[w] * word2weight[w] for w in tokens], axis=0)])
print(mean_vec_tfidf)

"""**ANN for vector index**"""

!apt install libomp-dev
!pip install faiss
import faiss

d = 512

index = faiss.IndexFlatL2(d)
print(index.is_trained)
index.add(mean_vec_transformer)
print(index.ntotal)

k = 4                          # we want to see 4 nearest neighbors
D, I = index.search(mean_vec_transformer, k) # sanity check
print(I)
print(D)
D, I = index.search(mean_vec_transformer, k)     # actual search
print(I[:5])                   # neighbors of the 5 first queries
print(I[-5:])                  # neighbors of the 5 last queries
