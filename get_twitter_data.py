# -*- coding: utf-8 -*-
"""Twitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sQ-TNsOoIyabl4bhst0A9SSv1YfleY0X
"""

# import lots of stuff
import sys
import os
import re
import tweepy
from tweepy import OAuthHandler

import numpy as np
import pandas as pd
from datetime import datetime, timedelta

from os import path
from PIL import Image
from wordcloud import WordCloud, STOPWORDS

from google.colab import drive
drive.mount('/content/gdrive')

#@title Enter Twitter Credentials
TWITTER_KEY = 'rxZKr5xZ9S1b6bG4jIVXVkZqu' #@param {type:"string"}
TWITTER_SECRET_KEY = 'fX1wkeXC9x7y9TcrZyBZx9b6LbVjh0500geu81ysMKpNSDkW2k' #@param {type:"string"}

auth = tweepy.AppAuthHandler(TWITTER_KEY, TWITTER_SECRET_KEY)

api = tweepy.API(auth, wait_on_rate_limit=True,
				   wait_on_rate_limit_notify=True)

if (not api):
    print ("Can't Authenticate")
    sys.exit(-1)

maxTweets = 1000 #@param {type:"slider", min:0, max:45000, step:100}
Filter_Retweets = True #@param {type:"boolean"}

tweet_list=[]
for tweet in tweepy.Cursor(api.user_timeline, id="jack", include_rts=False).items(maxTweets):
    #print(tweet.text)
    tweet_list.append([tweet.created_at.date(), 
                      tweet.id, tweet.user.screen_name, tweet.user.name, tweet.user.id, tweet.text, tweet.favorite_count, 
                      ])
    
#clear_output()
print("Downloaded {0} tweets".format(len(tweet_list)))

pd.set_option('display.max_colwidth', -1)

# load it into a pandas dataframe
tweet_df = pd.DataFrame(tweet_list, columns=['tweet_date', 'tweet_id', 'username', 'name', 'user_id', 'tweet', 'like_count'])
tweet_df.head()

tweet_df.to_csv('tweets.csv')
!cp tweets.csv "/content/gdrive/My Drive/"

#lower the words
tweet_df['tweet']=tweet_df['tweet'].str.lower()
tweet_df.head()

import nltk

#nltk.download()

#!python -m nltk.downloader all

#tokenize the words
from nltk.tokenize import word_tokenize

tokenized_words=[word_tokenize(tw) for tw in tweet_df['tweet']]

print(tokenized_words)

#tokenize the sentences

from nltk.tokenize import sent_tokenize

tokenized_sent=[sent_tokenize(tw) for tw in tweet_df['tweet']]

print(tokenized_sent)

#remove punctuations
import string

regex = re.compile('[%s]' % re.escape(string.punctuation))

tokenized_words_punc = []

for comment in tokenized_words:
  new_comment=[]
  for token in comment:
    new_token = regex.sub(u'', token)
    if not new_token==u'':
      new_comment.append(new_token)

  tokenized_words_punc.append(new_comment)

print(tokenized_words_punc)

#remove the stopwords
from nltk.corpus import stopwords

tokenized_words_stop=[]

for doc in tokenized_words_punc:
  new_term_vector=[]
  for word in doc:
    if not word in stopwords.words('english'):
      new_term_vector.append(word)

  tokenized_words_stop.append(new_term_vector)

print(tokenized_words_stop)

#Stemming of words
from nltk.stem.porter import PorterStemmer

porter=PorterStemmer()

preprocessed_docs=[]

for doc in tokenized_words_stop:
  final_doc=[]
  for word in doc:
    final_doc.append(porter.stem(word))
  
  preprocessed_docs.append(final_doc)

print(preprocessed_docs)

#Lemmatization of words
from nltk.stem.wordnet import WordNetLemmatizer

wordnet=WordNetLemmatizer()

processed_docs=[]

for doc in preprocessed_docs:
  last_doc=[]
  for word in doc:
    last_doc.append(wordnet.lemmatize(word))
  
  processed_docs.append(last_doc)

print(processed_docs)