import sys
import os
import re
import tweepy
from tweepy import OAuthHandler

import numpy as np
import pandas as pd

from os import path

import re
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances

import gensim.downloader as api
from gensim.models.word2vec import Word2Vec

!pip install sentence_transformers
!pip install tweet-preprocessor
!pip install unidecode
!python -m nltk.downloader all

from sentence_transformers import SentenceTransformer
import preprocessor as prep

import string
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
from unidecode import unidecode
from nltk.stem import WordNetLemmatizer
#from nltk.tokenize import word_tokenize
from nltk.tokenize import TweetTokenizer

from google.colab import drive
drive.mount('/content/gdrive')

auth = tweepy.AppAuthHandler(TWITTER_KEY, TWITTER_SECRET_KEY)

api = tweepy.API(auth, wait_on_rate_limit=True,
				   wait_on_rate_limit_notify=True)

if (not api):
    print ("Can't Authenticate")
    sys.exit(-1)

max_tweets = 1000

tweet_list=[]
for tweet in tweepy.Cursor(api.user_timeline, id="balajis", include_rts=False, tweet_mode="extended").items(max_tweets):

    tweet_list.append([ 
                      tweet.id, tweet.created_at.date(), tweet.user.screen_name, tweet.user.name, tweet.user.id, tweet.favorite_count, tweet.retweet_count, tweet.full_text
                      ])

print("Downloaded {0} tweets".format(len(tweet_list)))

pd.set_option('display.max_colwidth', -1)

# load it into a pandas dataframe
tweet_df = pd.DataFrame(tweet_list, columns=['tweet_id', 'tweet_date', 'username', 'name', 'user_id', 'like_count', 'retweet_count', 'text'])
tweet_df.head()

tweet_df.insert(loc=7, column='hashtag', value=tweet_df['text'].apply(lambda x: re.findall(r'\B#\w*[a-zA-Z]+\w*', x)))
tweet_df.head(10)

stopset = stopwords.words('english')
punctuations = '''!()-=![]{};:+`'"“”\,<>/@#$%^&*_~'''

prep.set_options(prep.OPT.URL, prep.OPT.MENTION)

cleaned = [prep.clean(text) for text in tweet_df['text']]

print(cleaned)

tweet_df['cleaned'] = cleaned

#tweet_df['cleaned'] = tweet_df.cleaned.apply(lambda x: " ".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stopset))

tweet_df['cleaned'] = tweet_df.cleaned.apply(lambda x: x.translate(str.maketrans(punctuations, ' '*len(punctuations))))

tweet_df.head(10)

tokenizer = TweetTokenizer()
lemmatizer = WordNetLemmatizer()
punctuations = '''!()-=![]{};:+`'"“”\,<>.?/@#$%^&*_~'''
stopset = stopwords.words('english')

abbrevs = {
    'dm': 'direct message',
    'ai':'artificial intelligence',
    'ar':'augmented reality',
    'vr':'virtual reality',
    'ml':'machine learning',
    'btc':'bitcoin',
    'eth':'ethereum'
}

def lemmatize(token):
    lemmatized_token1=lemmatizer.lemmatize(token, pos="v")
    lemmatized_token2=lemmatizer.lemmatize(lemmatized_token1, pos="n")
    lemmatized_token3=lemmatizer.lemmatize(lemmatized_token2, pos="a")
    lemmatized_token=lemmatizer.lemmatize(lemmatized_token3, pos="r")
    return lemmatized_token

def pre_process(corpus):
    corpus = corpus.lower()
    corpus = corpus.translate(str.maketrans(punctuations, ' '*len(punctuations)))
    tokens = tokenizer.tokenize(corpus)
    cleaned_corpus=[]
    for token in tokens:
      token = ''.join([i for i in token if not i.isdigit()])
      if ((token not in stopset) and (len(token)>2)):
        token = lemmatize(token)
        token = token.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))
        token = unidecode(token)
        cleaned_corpus.append(token)
      else:
        continue
    return cleaned_corpus

pre_processed = [pre_process(tweet) for tweet in cleaned]

print(pre_processed)

#Add cleaned text as new column
punctuations = '''0123456789!()-=![]{};:.?+`'“”"\,<>/@#$%^&*_~'''
#cleaned = [text.lower().translate(str.maketrans(punctuations, ' '*len(punctuations))) for text in cleaned]

#tweet_df['cleaned'] = cleaned
tweet_df['tokens'] = pre_processed

#tweet_new = tweet_df[tweet_df['cleaned'].str.split().str.len()>1]

tweet_df.head(10)

tweet_df.to_csv('tweets.csv')
!cp tweets.csv "/content/gdrive/My Drive/"
